{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "634bd083",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e4cec",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5311f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 1. Basic Libraries\n",
    "# ==============================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "import pkg_resources\n",
    "\n",
    "# ==============================\n",
    "# 2. Preprocessing & Pipeline\n",
    "# ==============================\n",
    "\n",
    "from numba import njit, prange\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    r2_score, mean_squared_error)\n",
    "\n",
    "# ==============================\n",
    "# 3. Outlier Detection\n",
    "# ==============================\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# ==============================\n",
    "# 4. Classic Models (Linear / Regression / Classification)\n",
    "# ==============================\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression, Ridge, Lasso, ElasticNet,\n",
    "    SGDClassifier, SGDRegressor, BayesianRidge,\n",
    "    PassiveAggressiveClassifier, PassiveAggressiveRegressor,\n",
    "    Perceptron, LinearRegression, HuberRegressor, Lars, LassoLars,\n",
    "    OrthogonalMatchingPursuit, ARDRegression, TweedieRegressor,\n",
    "    PoissonRegressor, GammaRegressor, QuantileRegressor, RidgeClassifier)\n",
    "\n",
    "# ==============================\n",
    "# 5. Tree-Based Models\n",
    "# ==============================\n",
    "\n",
    "from sklearn.tree import (\n",
    "    DecisionTreeClassifier, DecisionTreeRegressor,\n",
    "    ExtraTreeClassifier, ExtraTreeRegressor)\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, RandomForestRegressor,\n",
    "    ExtraTreesClassifier, ExtraTreesRegressor,\n",
    "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "    AdaBoostClassifier, AdaBoostRegressor,\n",
    "    BaggingClassifier, BaggingRegressor,\n",
    "    VotingClassifier, VotingRegressor,\n",
    "    StackingClassifier, StackingRegressor,\n",
    "    HistGradientBoostingClassifier, HistGradientBoostingRegressor)\n",
    "\n",
    "# ==============================\n",
    "# 6. Naive Bayes Models\n",
    "# ==============================\n",
    "\n",
    "from sklearn.naive_bayes import (\n",
    "    GaussianNB, BernoulliNB, MultinomialNB, ComplementNB)\n",
    "\n",
    "# ==============================\n",
    "# 7. Neighbors-Based Models\n",
    "# ==============================\n",
    "\n",
    "from sklearn.neighbors import (\n",
    "    KNeighborsClassifier, KNeighborsRegressor,\n",
    "    RadiusNeighborsClassifier, RadiusNeighborsRegressor,\n",
    "    NearestCentroid)\n",
    "\n",
    "# ==============================\n",
    "# 8. Support Vector Machines\n",
    "# ==============================\n",
    "\n",
    "from sklearn.svm import LinearSVC, LinearSVR\n",
    "\n",
    "# ==============================\n",
    "# 9. Neural Networks\n",
    "# ==============================\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "# ==============================\n",
    "# 10. Discriminant Analysis\n",
    "# ==============================\n",
    "\n",
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b371729a",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f05cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('features_30_sec.csv', low_memory=True)\n",
    "df.info(memory_usage='deep')\n",
    "target_col = \"label\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081363bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"Initial Data Overview:\")\n",
    "display(df.head())\n",
    "\n",
    "display(\"Column Names:\")\n",
    "display(df.columns)\n",
    "\n",
    "display(\"\\nMissing Values per Column:\")\n",
    "display(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a08fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\"filename\", \"length\"]\n",
    "cols_exist = [col for col in columns_to_remove if col in df.columns]\n",
    "\n",
    "if cols_exist:\n",
    "    print(f\"[INFO] Dropping columns: {cols_exist}\")\n",
    "    df = df.drop(columns=cols_exist)\n",
    "else:\n",
    "    print(\"[INFO] No columns to drop found in dataframe.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baebb45a",
   "metadata": {},
   "source": [
    "## 3. Missing Values Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305bd7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "missing_counts = df.isnull().sum()\n",
    "\n",
    "missing_counts = missing_counts[missing_counts > 0]\n",
    "\n",
    "if not missing_counts.empty:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    missing_counts.plot(kind='bar', color='orange')\n",
    "    plt.title(\"Missing Values per Column\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[INFO] No missing values found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555be94f",
   "metadata": {},
   "source": [
    "## 4. Handle Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da6ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "num_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "for col in cat_cols:\n",
    "    if df[col].isnull().any():\n",
    "        mode_val = df[col].mode()[0]\n",
    "        df[col].fillna(mode_val, inplace=True)\n",
    "        print(f\"Column {col} filled with Mode: {mode_val}\")\n",
    "\n",
    "for col in num_cols:\n",
    "    if df[col].isnull().any():\n",
    "        mean_val = df[col].mean()\n",
    "        df[col].fillna(mean_val, inplace=True)\n",
    "        print(f\"Column {col} filled with Mean: {mean_val:.2f}\")\n",
    "\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts = missing_counts[missing_counts > 0]\n",
    "\n",
    "if not missing_counts.empty:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    missing_counts.plot(kind='bar', color='green')\n",
    "    plt.title(\"Missing Values After Filling\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[INFO] No missing values remaining.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f24378",
   "metadata": {},
   "source": [
    "## 5. Remove Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b99d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicates = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {num_duplicates}\")\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print(\"Examples of duplicate rows:\")\n",
    "    display(df[df.duplicated()].head())\n",
    "    \n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(\"Duplicates removed\")\n",
    "else:\n",
    "    print(\"[INFO] No duplicate rows found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d1e02",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ea88fa",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis (Automatic Distributions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411da7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def plot_distributions_auto(df, sample_size=5000, max_unique_for_countplot=15, top_n_bar=10):\n",
    "    df_sample = df.sample(sample_size, random_state=42) if len(df) > sample_size else df\n",
    "    cols = df_sample.columns.tolist()\n",
    "    \n",
    "    n_cols = 3\n",
    "    n_rows = math.ceil(len(cols) / n_cols)\n",
    "\n",
    "    row_heights = []\n",
    "    for idx in range(n_rows):\n",
    "        row_cols = cols[idx*n_cols:(idx+1)*n_cols]\n",
    "        max_height = 4\n",
    "        for col in row_cols:\n",
    "            n_unique = df_sample[col].nunique()\n",
    "            dtype = df_sample[col].dtype\n",
    "            if dtype in [\"int64\", \"float64\"]:\n",
    "                if n_unique > 15:\n",
    "                    max_height = max(max_height, 4)\n",
    "                elif 3 < n_unique <= max_unique_for_countplot:\n",
    "                    max_height = max(max_height, 3)\n",
    "                else:\n",
    "                    max_height = max(max_height, 3.5)\n",
    "            else:\n",
    "                if n_unique == 2:\n",
    "                    max_height = max(max_height, 4)\n",
    "                elif n_unique <= max_unique_for_countplot:\n",
    "                    max_height = max(max_height, 4.5)\n",
    "                else:\n",
    "                    max_height = max(max_height, 5)\n",
    "        row_heights.append(max_height)\n",
    "\n",
    "    total_height = sum(row_heights) + 1\n",
    "    plt.figure(figsize=(n_cols*6, total_height))\n",
    "\n",
    "    # Plot each column\n",
    "    for idx, col in enumerate(cols):\n",
    "        plt.subplot(n_rows, n_cols, idx + 1)\n",
    "        n_unique = df_sample[col].nunique()\n",
    "        dtype = df_sample[col].dtype\n",
    "\n",
    "        # Automatic plot selection\n",
    "        if dtype in [\"int64\", \"float64\"]:\n",
    "            if n_unique > 15:\n",
    "                sns.histplot(df_sample[col], kde=True, bins=30)\n",
    "                plt.title(f\"{col} Histogram & KDE\")\n",
    "            elif 3 < n_unique <= max_unique_for_countplot:\n",
    "                sns.boxplot(x=df_sample[col])\n",
    "                plt.title(f\"{col} Boxplot\")\n",
    "            else:\n",
    "                sns.violinplot(y=df_sample[col])\n",
    "                plt.title(f\"{col} Violin\")\n",
    "        else:\n",
    "            if n_unique == 2:\n",
    "                df_sample[col].value_counts().plot.pie(autopct=\"%1.1f%%\", startangle=90)\n",
    "                plt.title(f\"{col} Pie\")\n",
    "                plt.ylabel(\"\")\n",
    "            elif n_unique <= max_unique_for_countplot:\n",
    "                sns.countplot(x=df_sample[col], order=df_sample[col].value_counts().index)\n",
    "                plt.title(f\"{col} Countplot\")\n",
    "                plt.xticks(rotation=45)\n",
    "            else:\n",
    "                top_vals = df_sample[col].value_counts().nlargest(top_n_bar)\n",
    "                sns.barplot(x=top_vals.index, y=top_vals.values)\n",
    "                plt.title(f\"{col} Top {top_n_bar}\")\n",
    "                plt.xticks(rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "plot_distributions_auto(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d40c53",
   "metadata": {},
   "source": [
    "## 7. Feature Relationship with Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99735398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_target_all(df, target_col, sample_size=1000, chunk_size=15):\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"Error: Target column '{target_col}' not found in dataframe.\")\n",
    "        print(f\"Available columns: {list(df.columns)}\")\n",
    "        return\n",
    "    \n",
    "    if len(df) > sample_size:\n",
    "        df_sample = df.sample(sample_size, random_state=42)\n",
    "        print(f\"[INFO] Using sample of {sample_size} rows for visualization\")\n",
    "    else:\n",
    "        df_sample = df.copy()\n",
    "    \n",
    "    features = [col for col in df_sample.columns if col != target_col]\n",
    "    total_features = len(features)\n",
    "    \n",
    "    print(f\"[INFO] Processing ALL {total_features} features in chunks of {chunk_size}\")\n",
    "    \n",
    "    for chunk_start in range(0, total_features, chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, total_features)\n",
    "        chunk_features = features[chunk_start:chunk_end]\n",
    "        \n",
    "        print(f\"\\n[INFO] Displaying features {chunk_start+1}-{chunk_end} of {total_features}\")\n",
    "        \n",
    "        n = len(chunk_features)\n",
    "        n_cols = 3 \n",
    "        n_rows = math.ceil(n / n_cols)\n",
    "        \n",
    "        row_height = 4\n",
    "        \n",
    "        plt.figure(figsize=(18, n_rows * row_height))\n",
    "        \n",
    "        for i, col in enumerate(chunk_features):\n",
    "            plt.subplot(n_rows, n_cols, i + 1)\n",
    "            \n",
    "            try:\n",
    "                if df_sample[col].dtype in [\"int64\", \"float64\"] and df_sample[target_col].dtype == \"object\":\n",
    "                    if df_sample[target_col].nunique() <= 10:\n",
    "                        sns.boxplot(x=df_sample[target_col], y=df_sample[col])\n",
    "                        plt.title(f\"{col} vs {target_col}\")\n",
    "                    else:\n",
    "                        sns.violinplot(x=df_sample[target_col], y=df_sample[col])\n",
    "                        plt.title(f\"{col} vs {target_col}\")\n",
    "                \n",
    "                elif df_sample[col].dtype == \"object\" and df_sample[target_col].dtype == \"object\":\n",
    "                    if df_sample[col].nunique() <= 10 and df_sample[target_col].nunique() <= 10:\n",
    "                        sns.countplot(x=df_sample[col], hue=df_sample[target_col])\n",
    "                        plt.title(f\"{col} vs {target_col}\")\n",
    "                    else:\n",
    "                        top_vals = df_sample[col].value_counts().nlargest(10).index\n",
    "                        filtered_data = df_sample[df_sample[col].isin(top_vals)]\n",
    "                        sns.countplot(data=filtered_data, x=col, hue=target_col)\n",
    "                        plt.title(f\"Top 10 {col} vs {target_col}\")\n",
    "                        plt.xticks(rotation=45)\n",
    "                \n",
    "                elif df_sample[col].dtype in [\"int64\",\"float64\"] and df_sample[target_col].dtype in [\"int64\",\"float64\"]:\n",
    "                    sns.scatterplot(x=df_sample[col], y=df_sample[target_col], alpha=0.6)\n",
    "                    plt.title(f\"{col} vs {target_col}\")\n",
    "                \n",
    "                elif df_sample[col].dtype == \"object\" and df_sample[target_col].dtype in [\"int64\",\"float64\"]:\n",
    "                    if df_sample[col].nunique() <= 15:\n",
    "                        sns.boxplot(x=df_sample[col], y=df_sample[target_col])\n",
    "                        plt.title(f\"{col} vs {target_col}\")\n",
    "                        plt.xticks(rotation=45)\n",
    "                    else:\n",
    "                        top_cats = df_sample[col].value_counts().nlargest(10).index\n",
    "                        filtered_data = df_sample[df_sample[col].isin(top_cats)]\n",
    "                        sns.boxplot(data=filtered_data, x=col, y=target_col)\n",
    "                        plt.title(f\"Top 10 {col} vs {target_col}\")\n",
    "                        plt.xticks(rotation=45)\n",
    "            \n",
    "            except Exception as e:\n",
    "                plt.text(0.5, 0.5, f\"Error plotting {col}\\n{str(e)[:50]}...\", \n",
    "                        transform=plt.gca().transAxes, ha='center', va='center')\n",
    "                plt.title(f\"{col} (Error)\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"\\n[INFO] Displayed ALL {total_features} features\")\n",
    "\n",
    "compare_with_target_all(df, \"label\", sample_size=1000, chunk_size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7372d09d",
   "metadata": {},
   "source": [
    "## 8. Feature Relationships Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be352c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def compare_features_auto(df, sample_size=5000, max_pairplots=500, max_categories=10, n_cols=3):\n",
    "    numeric_cols = df.select_dtypes(include=[\"int64\",\"float64\"]).columns\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\",\"category\"]).columns\n",
    "\n",
    "    df_sample = df.sample(min(sample_size, len(df)), random_state=42)\n",
    "\n",
    "    # 1. Correlation Heatmap\n",
    "    if len(numeric_cols) > 1:\n",
    "        plt.figure(figsize=(12,10))\n",
    "        sns.heatmap(df_sample[numeric_cols].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\",\n",
    "                    annot_kws={\"size\":8})\n",
    "        plt.title(\"Correlation Heatmap (Numeric Features)\", fontsize=16)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if len(numeric_cols) >= 2:\n",
    "        pairs = list(combinations(numeric_cols, 2))[:max_pairplots]\n",
    "        chunk_size = n_cols * 3  \n",
    "        for i in range(0, len(pairs), chunk_size):\n",
    "            chunk = pairs[i:i+chunk_size]\n",
    "            n_rows = int(np.ceil(len(chunk)/n_cols))\n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*4))\n",
    "            axes = np.array(axes).flatten()\n",
    "            for idx, (col1, col2) in enumerate(chunk):\n",
    "                sns.scatterplot(x=df_sample[col1], y=df_sample[col2], ax=axes[idx])\n",
    "                axes[idx].set_title(f\"{col1} vs {col2}\", fontsize=10)\n",
    "            for j in range(len(chunk), len(axes)):\n",
    "                axes[j].set_visible(False)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    if len(categorical_cols) >= 2:\n",
    "        cat_pairs = list(combinations(categorical_cols, 2))\n",
    "        chunk_size = n_cols * 3\n",
    "        for i in range(0, len(cat_pairs), chunk_size):\n",
    "            chunk = cat_pairs[i:i+chunk_size]\n",
    "            n_rows = int(np.ceil(len(chunk)/n_cols))\n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*4))\n",
    "            axes = np.array(axes).flatten()\n",
    "            for idx, (col1, col2) in enumerate(chunk):\n",
    "                top1 = df_sample[col1].value_counts().nlargest(max_categories).index\n",
    "                top2 = df_sample[col2].value_counts().nlargest(max_categories).index\n",
    "                crosstab = pd.crosstab(df_sample[col1].where(df_sample[col1].isin(top1)),\n",
    "                                       df_sample[col2].where(df_sample[col2].isin(top2)))\n",
    "                if crosstab.empty:\n",
    "                    axes[idx].set_visible(False)\n",
    "                    continue\n",
    "                sns.heatmap(crosstab, annot=False, cmap=\"Blues\", ax=axes[idx])\n",
    "                axes[idx].set_title(f\"{col1} vs {col2}\", fontsize=10)\n",
    "                axes[idx].tick_params(axis='x', rotation=45)\n",
    "                axes[idx].tick_params(axis='y', rotation=0)\n",
    "            for j in range(len(chunk), len(axes)):\n",
    "                axes[j].set_visible(False)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Usage\n",
    "compare_features_auto(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c826f95",
   "metadata": {},
   "source": [
    "## 9. Numeric Features vs Target Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f975fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix target column name\n",
    "target_col = \"label\"\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def scatter_features_with_target_dynamic(df, target_col, sample_size=5000, n_cols=3):\n",
    "    numeric_cols = [col for col in df.select_dtypes(include=[\"int64\",\"float64\"]).columns if col != target_col]\n",
    "    if not numeric_cols:\n",
    "        print(\"No numeric features found.\")\n",
    "        return\n",
    "\n",
    "    df_sample = df.sample(sample_size, random_state=42) if len(df) > sample_size else df\n",
    "\n",
    "    n_rows = math.ceil(len(numeric_cols) / n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*4))\n",
    "    axes = np.array(axes).flatten()\n",
    "\n",
    "    for idx, col in enumerate(numeric_cols):\n",
    "        ax = axes[idx]\n",
    "        if df_sample[target_col].dtype in [\"int64\",\"float64\"]:\n",
    "            sns.regplot(x=df_sample[col], y=df_sample[target_col], scatter_kws={\"alpha\":0.5}, ax=ax)\n",
    "            ax.set_title(f\"{col} vs {target_col} Scatter + Reg\")\n",
    "        elif df_sample[target_col].dtype == \"object\":\n",
    "            sns.boxplot(x=df_sample[target_col], y=df_sample[col], ax=ax)\n",
    "            sns.stripplot(x=df_sample[target_col], y=df_sample[col], color=\"black\", alpha=0.3, ax=ax)\n",
    "            ax.set_title(f\"{col} vs {target_col} Box + Strip\")\n",
    "\n",
    "    for i in range(len(numeric_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "scatter_features_with_target_dynamic(df, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e63dc0",
   "metadata": {},
   "source": [
    "## 10. Self Comparison of Numeric Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a783509",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def self_comparison_dynamic(df, sample_size=5000, n_cols=3):\n",
    "    numeric_cols = df.select_dtypes(include=[\"int64\",\"float64\"]).columns\n",
    "    if numeric_cols.empty:\n",
    "        print(\"No numeric features found.\")\n",
    "        return\n",
    "\n",
    "    # Sampling\n",
    "    df_sample = df.sample(sample_size, random_state=42) if len(df) > sample_size else df\n",
    "\n",
    "    plot_types = [\"hist_kde\", \"box\", \"violin\", \"ecdf\", \"qq\"]\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        n_plots = len(plot_types)\n",
    "        n_rows = math.ceil(n_plots / n_cols)\n",
    "        fig, axes = plt.subplots(n_rows, min(n_cols, n_plots), figsize=(n_cols*5, n_rows*4))\n",
    "        axes = np.array(axes).flatten()\n",
    "\n",
    "        for idx, plot_type in enumerate(plot_types):\n",
    "            ax = axes[idx]\n",
    "            if plot_type == \"hist_kde\":\n",
    "                sns.histplot(df_sample[col], kde=True, bins=30, color=\"steelblue\", ax=ax)\n",
    "                ax.set_title(f\"Histogram & KDE of {col}\")\n",
    "            elif plot_type == \"box\":\n",
    "                sns.boxplot(x=df_sample[col], color=\"orange\", ax=ax)\n",
    "                ax.set_title(f\"Boxplot of {col}\")\n",
    "            elif plot_type == \"violin\":\n",
    "                sns.violinplot(y=df_sample[col], color=\"purple\", ax=ax)\n",
    "                ax.set_title(f\"Violin Plot of {col}\")\n",
    "            elif plot_type == \"ecdf\":\n",
    "                sns.ecdfplot(df_sample[col], color=\"green\", ax=ax)\n",
    "                ax.set_title(f\"ECDF of {col}\")\n",
    "            elif plot_type == \"qq\":\n",
    "                stats.probplot(df_sample[col], dist=\"norm\", plot=ax)\n",
    "                ax.set_title(f\"Q-Q Plot of {col}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Usage\n",
    "self_comparison_dynamic(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b978e3c4",
   "metadata": {},
   "source": [
    "# Feature Engineering & Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62a84a4",
   "metadata": {},
   "source": [
    "## 11. Encode Categorical Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9311f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataframe(df: pd.DataFrame, target_col: str):\n",
    "    df_encoded = df.copy()\n",
    "\n",
    "    if df_encoded[target_col].dtype == 'object' or df_encoded[target_col].dtype.name == 'category':\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[target_col] = le.fit_transform(df_encoded[target_col])\n",
    "        print(f\"[INFO] Target column '{target_col}' encoded: {list(le.classes_)} -> {list(range(len(le.classes_)))}\")\n",
    "\n",
    "    categorical_cols = df_encoded.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    if target_col in categorical_cols:\n",
    "        categorical_cols.remove(target_col)\n",
    "\n",
    "    if categorical_cols:\n",
    "        print(f\"[INFO] Encoding categorical columns: {categorical_cols}\")\n",
    "        df_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, drop_first=True)\n",
    "    else:\n",
    "        print(\"[INFO] No categorical columns to encode.\")\n",
    "\n",
    "    return df_encoded\n",
    "df = encode_dataframe(df, target_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad82a301",
   "metadata": {},
   "source": [
    "\n",
    "## 12. Smart Preprocessing, Outlier Removal, and Data Balancing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f147541",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from imblearn.over_sampling import SMOTE, SMOTENC, RandomOverSampler\n",
    "    _IMBLEARN_OK = True\n",
    "except:\n",
    "    _IMBLEARN_OK = False\n",
    "\n",
    "# ====================== Fast LOF with Numba ======================\n",
    "@njit(parallel=True)\n",
    "def pairwise_distance(X):\n",
    "    n_samples = X.shape[0]\n",
    "    distances = np.empty((n_samples, n_samples), dtype=np.float32)\n",
    "    for i in prange(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            diff = X[i] - X[j]\n",
    "            distances[i, j] = np.sqrt(np.dot(diff, diff))\n",
    "    return distances\n",
    "\n",
    "@njit\n",
    "def local_reachability_density(distances, k):\n",
    "    n_samples = distances.shape[0]\n",
    "    lrd = np.zeros(n_samples, dtype=np.float32)\n",
    "    for i in range(n_samples):\n",
    "        sorted_idx = np.argsort(distances[i])\n",
    "        neighbors_idx = sorted_idx[1:k+1]\n",
    "        reach_dist_sum = 0.0\n",
    "        for j in neighbors_idx:\n",
    "            k_dist_j = distances[j][np.argsort(distances[j])[k]]\n",
    "            reach_dist = max(k_dist_j, distances[i, j])\n",
    "            reach_dist_sum += reach_dist\n",
    "        lrd[i] = k / reach_dist_sum if reach_dist_sum > 0 else 0\n",
    "    return lrd\n",
    "\n",
    "@njit\n",
    "def lof_score(lrd, distances, k):\n",
    "    n_samples = distances.shape[0]\n",
    "    lof = np.zeros(n_samples, dtype=np.float32)\n",
    "    for i in range(n_samples):\n",
    "        sorted_idx = np.argsort(distances[i])\n",
    "        neighbors_idx = sorted_idx[1:k+1]\n",
    "        sum_ratio = 0.0\n",
    "        for j in neighbors_idx:\n",
    "            if lrd[i] > 0:\n",
    "                sum_ratio += lrd[j] / lrd[i]\n",
    "        lof[i] = sum_ratio / k\n",
    "    return lof\n",
    "\n",
    "def smart_lof_mask(df, numeric_cols, k=20, threshold=1.5):\n",
    "    X = df[numeric_cols].fillna(df[numeric_cols].median()).values.astype(np.float32)\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    distances = pairwise_distance(X)\n",
    "    lrd = local_reachability_density(distances, k)\n",
    "    lof = lof_score(lrd, distances, k)\n",
    "    mask = lof <= threshold\n",
    "    print(f\"[INFO] LOF: removed {(~mask).sum()} rows ({100*(~mask).sum()/len(df):.2f}%)\")\n",
    "    return mask\n",
    "\n",
    "# ====================== Smart Preprocessing + Balancing ======================\n",
    "def smart_preprocess_balance_fast(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    drop_cols: list = None,\n",
    "    outlier_contamination: float = 0.01,\n",
    "    balance_method: str = \"smote\",\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42\n",
    "):\n",
    "    df2 = df.copy()\n",
    "    if drop_cols:\n",
    "        df2.drop(columns=[c for c in drop_cols if c in df2.columns], inplace=True)\n",
    "\n",
    "    if target_col not in df2.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found.\")\n",
    "\n",
    "    cat_cols = df2.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "    if target_col in cat_cols:\n",
    "        cat_cols.remove(target_col)\n",
    "    num_cols = df2.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col in num_cols:\n",
    "        num_cols.remove(target_col)\n",
    "\n",
    "    print(f\"[INFO] Categorical cols: {cat_cols}\")\n",
    "    print(f\"[INFO] Numerical cols: {num_cols}\")\n",
    "\n",
    "    # ---- Outlier Removal ----\n",
    "    if len(num_cols) > 0:\n",
    "        n_rows, n_cols_df = df2.shape\n",
    "        method_used = None\n",
    "        if n_rows > 10000:  \n",
    "            print(\"[INFO] Using IsolationForest for outlier removal\")\n",
    "            iso = IsolationForest(contamination=outlier_contamination, random_state=random_state)\n",
    "            mask = iso.fit_predict(df2[num_cols].fillna(df2[num_cols].median())) != -1\n",
    "            method_used = \"IsolationForest\"\n",
    "        elif n_cols_df <= 5:  \n",
    "            print(\"[INFO] Using EllipticEnvelope for outlier removal\")\n",
    "            env = EllipticEnvelope(contamination=outlier_contamination, random_state=random_state)\n",
    "            mask = env.fit_predict(df2[num_cols].fillna(df2[num_cols].median())) != -1\n",
    "            method_used = \"EllipticEnvelope\"\n",
    "        else:\n",
    "            print(\"[INFO] Using fast LOF for outlier removal\")\n",
    "            mask = smart_lof_mask(df2, num_cols)\n",
    "            method_used = \"LOF\"\n",
    "\n",
    "        df2 = df2.loc[mask].reset_index(drop=True)\n",
    "        print(f\"[INFO] Outlier removal done using {method_used}, new shape: {df2.shape}\")\n",
    "\n",
    "    # ---- Preprocessing Pipelines ----\n",
    "    num_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    cat_transformer = None\n",
    "    if len(cat_cols) > 0:\n",
    "        cat_transformer = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True))\n",
    "        ])\n",
    "\n",
    "    transformers = [(\"num\", num_transformer, num_cols)]\n",
    "    if cat_transformer:\n",
    "        transformers.append((\"cat\", cat_transformer, cat_cols))\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "\n",
    "    X = df2.drop(columns=[target_col])\n",
    "    y = df2[target_col]\n",
    "\n",
    "    # ---- Detect Task Type ----\n",
    "    if (pd.api.types.is_integer_dtype(y) or pd.api.types.is_object_dtype(y)) and y.nunique() < 20:\n",
    "        task_type = \"classification\"\n",
    "        if pd.api.types.is_object_dtype(y):\n",
    "            y = y.astype('category').cat.codes\n",
    "    else:\n",
    "        task_type = \"regression\"\n",
    "    print(f\"[INFO] Detected task type: {task_type}\")\n",
    "\n",
    "    # ---- Split ----\n",
    "    if task_type == \"classification\":\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "        print(\"[INFO] Stratified split applied.\")\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state)\n",
    "        print(\"[INFO] Normal split applied.\")\n",
    "\n",
    "    # ---- Preprocess ----\n",
    "    X_train_proc = preprocessor.fit_transform(X_train)\n",
    "    X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "    # ---- Feature Names ----\n",
    "    feat_names = None\n",
    "    try:\n",
    "        cat_names = []\n",
    "        if cat_transformer:\n",
    "            cat_names = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(cat_cols)\n",
    "        feat_names = np.concatenate([num_cols, cat_names])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # ---- Balancing ----\n",
    "    if task_type == \"classification\" and _IMBLEARN_OK and balance_method.lower() != \"none\":\n",
    "        sampler = None\n",
    "        if balance_method.lower() == \"smote\":\n",
    "            if len(cat_cols) > 0:\n",
    "                try:\n",
    "                    cat_idx = list(range(len(num_cols), X_train_proc.shape[1]))\n",
    "                    sampler = SMOTENC(categorical_features=cat_idx, random_state=random_state, n_jobs=-1)\n",
    "                except Exception:\n",
    "                    sampler = SMOTE(random_state=random_state, n_jobs=-1)\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=random_state, n_jobs=-1)\n",
    "        elif balance_method.lower() in [\"ros\", \"randomoversampler\"]:\n",
    "            sampler = RandomOverSampler(random_state=random_state)\n",
    "\n",
    "        if sampler:\n",
    "            X_train_proc, y_train = sampler.fit_resample(X_train_proc, y_train)\n",
    "            print(f\"[INFO] Balanced training set: {len(y_train)} samples\")\n",
    "\n",
    "    return X_train_proc, X_test_proc, y_train, y_test, preprocessor, feat_names\n",
    "\n",
    "X_train_proc, X_test_proc, y_train, y_test, preprocessor, feat_names = smart_preprocess_balance_fast(\n",
    "    df=df,\n",
    "    target_col=target_col,\n",
    "    outlier_contamination=0.01,\n",
    "    balance_method=\"smote\",\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "if feat_names is not None:\n",
    "    X_train_df = pd.DataFrame(\n",
    "        X_train_proc.toarray() if hasattr(X_train_proc, 'toarray') else X_train_proc,\n",
    "        columns=feat_names\n",
    "    )\n",
    "    print(X_train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28d10b",
   "metadata": {},
   "source": [
    "# Automated Machine Learning Pipeline (AutoML Ultra)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd784c",
   "metadata": {},
   "source": [
    "## 13. Full AutoML: Problem Detection, Model Training, and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2650f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional models\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "    _LGBM_OK = True\n",
    "except: _LGBM_OK = False\n",
    "try:\n",
    "    from xgboost import XGBClassifier, XGBRegressor\n",
    "    _XGB_OK = True\n",
    "except: _XGB_OK = False\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "    _CAT_OK = True\n",
    "except: _CAT_OK = False\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    _PROPHET_OK = True\n",
    "except: _PROPHET_OK = False\n",
    "try:\n",
    "    from pmdarima import auto_arima\n",
    "    _PMDARIMA_OK = True\n",
    "except: _PMDARIMA_OK = False\n",
    "\n",
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def numba_argsort(arr):\n",
    "    return np.argsort(arr)\n",
    "\n",
    "def print_step(msg):\n",
    "    print(f\"\\n{'='*10} {msg} {'='*10}\")\n",
    "\n",
    "def detect_problem_type(df, target_col, time_col=None):\n",
    "    if time_col and time_col in df.columns:\n",
    "        try:\n",
    "            pd.to_datetime(df[time_col])\n",
    "            return \"time_series\"\n",
    "        except: pass\n",
    "    y = df[target_col]\n",
    "    if pd.api.types.is_numeric_dtype(y):\n",
    "        if (y.dropna() % 1 == 0).all() and y.nunique(dropna=True) <= 20:\n",
    "            return \"classification\"\n",
    "        else:\n",
    "            return \"regression\"\n",
    "    return \"classification\"\n",
    "\n",
    "def preprocess_data(df, target_col, time_col=None):\n",
    "    df = df.copy()\n",
    "    if time_col and time_col in df.columns:\n",
    "        df = df.sort_values(time_col)\n",
    "    cat_cols = df.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col in cat_cols: cat_cols.remove(target_col)\n",
    "    if target_col in num_cols: num_cols.remove(target_col)\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
    "    ]) if cat_cols else \"drop\"\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols)\n",
    "    ])\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    X_proc = preprocessor.fit_transform(X)\n",
    "    feat_names = []\n",
    "    feat_names.extend(num_cols)\n",
    "    if cat_cols:\n",
    "        feat_names.extend(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(cat_cols))\n",
    "    return X_proc, y, feat_names\n",
    "\n",
    "def get_models(task, n_samples=None):\n",
    "    # Reduce max_iter for heavy models if data is large\n",
    "    heavy_iter = 200 if n_samples is not None and n_samples > 50000 else 2000\n",
    "    mlp_iter = 100 if n_samples is not None and n_samples > 50000 else 300\n",
    "    lgbm_estimators = 50 if n_samples is not None and n_samples > 50000 else 200\n",
    "    xgb_estimators = 50 if n_samples is not None and n_samples > 50000 else 200\n",
    "    cat_estimators = 50 if n_samples is not None and n_samples > 50000 else 200\n",
    "\n",
    "    models = []\n",
    "    if task == \"classification\":\n",
    "        models += [\n",
    "            (\"RandomForest\", RandomForestClassifier(n_estimators=lgbm_estimators, n_jobs=-1, random_state=42)),\n",
    "            (\"ExtraTrees\", ExtraTreesClassifier(n_estimators=lgbm_estimators, n_jobs=-1, random_state=42)),\n",
    "            (\"GradientBoosting\", GradientBoostingClassifier(n_estimators=lgbm_estimators, random_state=42)),\n",
    "            (\"HistGB\", HistGradientBoostingClassifier(max_iter=lgbm_estimators, random_state=42)),\n",
    "            (\"DecisionTree\", DecisionTreeClassifier(random_state=42)),\n",
    "            (\"ExtraTree\", ExtraTreeClassifier(random_state=42)),\n",
    "            (\"AdaBoost\", AdaBoostClassifier(n_estimators=lgbm_estimators, random_state=42)),\n",
    "            (\"Bagging\", BaggingClassifier(n_estimators=100, n_jobs=-1, random_state=42)),\n",
    "            (\"LogisticRegression\", LogisticRegression(max_iter=heavy_iter, n_jobs=-1, random_state=42)),\n",
    "            (\"SGDClassifier\", SGDClassifier(max_iter=heavy_iter, random_state=42)),\n",
    "            (\"LinearSVC\", LinearSVC(max_iter=heavy_iter, random_state=42)),\n",
    "            (\"RidgeClassifier\", RidgeClassifier()) ,  \n",
    "            (\"PassiveAggressiveClassifier\", PassiveAggressiveClassifier(max_iter=1000, random_state=42)),\n",
    "            (\"Perceptron\", Perceptron(max_iter=1000, random_state=42)),\n",
    "            (\"GaussianNB\", GaussianNB()),\n",
    "            (\"BernoulliNB\", BernoulliNB()),\n",
    "            (\"MultinomialNB\", MultinomialNB()),\n",
    "            (\"ComplementNB\", ComplementNB()),\n",
    "            (\"LDA\", LinearDiscriminantAnalysis()),\n",
    "            (\"QDA\", QuadraticDiscriminantAnalysis()),\n",
    "            (\"KNN\", KNeighborsClassifier(n_jobs=-1)),\n",
    "            (\"RadiusNN\", RadiusNeighborsClassifier(n_jobs=-1, outlier_label=0)),\n",
    "            (\"NearestCentroid\", NearestCentroid()),\n",
    "            (\"MLP\", MLPClassifier(hidden_layer_sizes=(64,32), max_iter=mlp_iter, random_state=42)),\n",
    "        ]\n",
    "        if _LGBM_OK: models.append((\"LightGBM\", LGBMClassifier(n_estimators=lgbm_estimators, n_jobs=-1, random_state=42)))\n",
    "        if _XGB_OK:  models.append((\"XGBoost\", XGBClassifier(n_estimators=xgb_estimators, n_jobs=-1, random_state=42, verbosity=0, use_label_encoder=False)))\n",
    "        if _CAT_OK:  models.append((\"CatBoost\", CatBoostClassifier(n_estimators=cat_estimators, verbose=0, random_state=42)))\n",
    "    else:\n",
    "        models += [\n",
    "            (\"RandomForest\", RandomForestRegressor(n_estimators=lgbm_estimators, n_jobs=-1, random_state=42)),\n",
    "            (\"ExtraTrees\", ExtraTreesRegressor(n_estimators=lgbm_estimators, n_jobs=-1, random_state=42)),\n",
    "            (\"GradientBoosting\", GradientBoostingRegressor(n_estimators=lgbm_estimators, random_state=42)),\n",
    "            (\"HistGB\", HistGradientBoostingRegressor(max_iter=lgbm_estimators, random_state=42)),\n",
    "            (\"DecisionTree\", DecisionTreeRegressor(random_state=42)),\n",
    "            (\"ExtraTree\", ExtraTreeRegressor(random_state=42)),\n",
    "            (\"AdaBoost\", AdaBoostRegressor(n_estimators=lgbm_estimators, random_state=42)),\n",
    "            (\"Bagging\", BaggingRegressor(n_estimators=100, n_jobs=-1, random_state=42)),\n",
    "            (\"LinearRegression\", LinearRegression()),\n",
    "            (\"Ridge\", Ridge()),\n",
    "            (\"Lasso\", Lasso()),\n",
    "            (\"ElasticNet\", ElasticNet()),\n",
    "            (\"SGDRegressor\", SGDRegressor(max_iter=heavy_iter, random_state=42)),\n",
    "            (\"BayesianRidge\", BayesianRidge()),\n",
    "            (\"HuberRegressor\", HuberRegressor()),\n",
    "            (\"Lars\", Lars()),\n",
    "            (\"LassoLars\", LassoLars()),\n",
    "            (\"OrthogonalMatchingPursuit\", OrthogonalMatchingPursuit()),\n",
    "            (\"ARDRegression\", ARDRegression()),\n",
    "            (\"TweedieRegressor\", TweedieRegressor()),\n",
    "            (\"PoissonRegressor\", PoissonRegressor()),\n",
    "            (\"GammaRegressor\", GammaRegressor()),\n",
    "            (\"QuantileRegressor\", QuantileRegressor()),\n",
    "            (\"PassiveAggressiveRegressor\", PassiveAggressiveRegressor(max_iter=1000, random_state=42)),\n",
    "            (\"KNN\", KNeighborsRegressor(n_jobs=-1)),\n",
    "            (\"RadiusNN\", RadiusNeighborsRegressor(n_jobs=-1)),\n",
    "            (\"LinearSVR\", LinearSVR(max_iter=heavy_iter, random_state=42)),\n",
    "            (\"MLP\", MLPRegressor(hidden_layer_sizes=(64,32), max_iter=mlp_iter, random_state=42)),\n",
    "        ]\n",
    "        if _LGBM_OK: models.append((\"LightGBM\", LGBMRegressor(n_estimators=lgbm_estimators, n_jobs=-1, random_state=42)))\n",
    "        if _XGB_OK:  models.append((\"XGBoost\", XGBRegressor(n_estimators=xgb_estimators, n_jobs=-1, random_state=42, verbosity=0)))\n",
    "        if _CAT_OK:  models.append((\"CatBoost\", CatBoostRegressor(n_estimators=cat_estimators, verbose=0, random_state=42)))\n",
    "    return models\n",
    "\n",
    "def auto_ml_ultra(df, target_col, time_col=None, min_score=0.88):\n",
    "    import joblib\n",
    "    t0 = time.time()\n",
    "    print_step(\"Detecting problem type\")\n",
    "    task = detect_problem_type(df, target_col, time_col)\n",
    "    print(f\"[INFO] Task: {task}\")\n",
    "\n",
    "    if task == \"time_series\":\n",
    "        if _PROPHET_OK:\n",
    "            print_step(\"Prophet Forecasting\")\n",
    "            df2 = df[[time_col, target_col]].rename(columns={time_col: \"ds\", target_col: \"y\"})\n",
    "            m = Prophet()\n",
    "            m.fit(df2)\n",
    "            future = m.make_future_dataframe(periods=10)\n",
    "            forecast = m.predict(future)\n",
    "            print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())\n",
    "        if _PMDARIMA_OK:\n",
    "            print_step(\"ARIMA Forecasting\")\n",
    "            model = auto_arima(df[target_col], seasonal=False, trace=True)\n",
    "            print(model.summary())\n",
    "        print(\"[INFO] Time Series task finished.\")\n",
    "        return\n",
    "\n",
    "    print_step(\"Preprocessing\")\n",
    "    X_proc, y, feat_names = preprocess_data(df, target_col, time_col)\n",
    "    print(f\"[INFO] Features: {X_proc.shape[1]} | Samples: {X_proc.shape[0]}\")\n",
    "\n",
    "    print_step(\"Splitting data\")\n",
    "    if task == \"classification\":\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_proc, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        score_metric = \"accuracy\"\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_proc, y, test_size=0.2, random_state=42)\n",
    "        cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        score_metric = \"r2\"\n",
    "    print(f\"[INFO] Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "    print_step(\"Model selection & training\")\n",
    "    models = get_models(task, n_samples=X_train.shape[0])\n",
    "    results = []\n",
    "    results_table = []\n",
    "    for idx, (name, model) in enumerate(models):\n",
    "        t1 = time.time()\n",
    "        try:\n",
    "            scores = cross_val_score(model, X_train, y_train, scoring=score_metric, cv=cv, n_jobs=-1)\n",
    "            score = scores.mean()\n",
    "            std = scores.std()\n",
    "            print(f\"[{idx+1:02d}/{len(models)}] {name}: CV {score_metric} = {score:.4f} ± {std:.4f} | Time: {time.time()-t1:.1f}s\")\n",
    "            results.append((score, name, model))\n",
    "            results_table.append({\n",
    "                \"Model\": name,\n",
    "                \"CV_Mean\": score,\n",
    "                \"CV_Std\": std,\n",
    "                \"Time_sec\": time.time()-t1\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx+1:02d}/{len(models)}] {name}: ERROR {e}\")\n",
    "            results_table.append({\n",
    "                \"Model\": name,\n",
    "                \"CV_Mean\": np.nan,\n",
    "                \"CV_Std\": np.nan,\n",
    "                \"Time_sec\": time.time()-t1,\n",
    "                \"Error\": str(e)\n",
    "            })\n",
    "    results.sort(reverse=True)\n",
    "    top_models = results[:10]\n",
    "    best_score, best_name, best_model = top_models[0]\n",
    "\n",
    "    # Voting/Stacking\n",
    "    print_step(\"Ensemble (Voting/Stacking)\")\n",
    "    try:\n",
    "    \n",
    "     use_stacking = X_train.shape[0] <= 100000  \n",
    "\n",
    "     if use_stacking:\n",
    "        print(\" Stacking Enabled (dataset is small enough)\")\n",
    "     else:\n",
    "        print(\"  Stacking Disabled (dataset too large, using Voting only)\")\n",
    "\n",
    "     if task == \"classification\":\n",
    "        voting = VotingClassifier(\n",
    "            estimators=[(n, m) for _, n, m in top_models],\n",
    "            voting='soft',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        if use_stacking:\n",
    "            stacking = StackingClassifier(\n",
    "                estimators=[(n, m) for _, n, m in top_models],\n",
    "                final_estimator=LogisticRegression(max_iter=2000),\n",
    "                n_jobs=-1\n",
    "            )\n",
    "     else:\n",
    "        voting = VotingRegressor(\n",
    "            estimators=[(n, m) for _, n, m in top_models],\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        if use_stacking:\n",
    "            stacking = StackingRegressor(\n",
    "                estimators=[(n, m) for _, n, m in top_models],\n",
    "                final_estimator=Ridge(),\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "     ensembles = [(\"Voting\", voting)]\n",
    "     if use_stacking:\n",
    "        ensembles.append((\"Stacking\", stacking))\n",
    "\n",
    "     for ens_name, ens_model in ensembles:\n",
    "        t1 = time.time()\n",
    "        scores = cross_val_score(\n",
    "            ens_model, X_train, y_train,\n",
    "            scoring=score_metric, cv=cv, n_jobs=-1\n",
    "        )\n",
    "        score = scores.mean()\n",
    "        std = scores.std()\n",
    "        elapsed = time.time() - t1\n",
    "\n",
    "        print(f\"[Ensemble] {ens_name}: CV {score_metric} = {score:.4f} ± {std:.4f} | Time: {elapsed:.1f}s\")\n",
    "\n",
    "        results.append((score, ens_name, ens_model))\n",
    "        results_table.append({\n",
    "            \"Model\": ens_name,\n",
    "            \"CV_Mean\": score,\n",
    "            \"CV_Std\": std,\n",
    "            \"Time_sec\": elapsed\n",
    "        })\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score, best_name, best_model = score, ens_name, ens_model\n",
    "\n",
    "     print(f\"\\n Best Ensemble Model: {best_name} with CV {score_metric} = {best_score:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "     print(f\"Ensemble Error: {e}\")\n",
    "\n",
    "\n",
    "    print_step(f\"Best Model: {best_name} (CV {score_metric}: {best_score:.4f})\")\n",
    "    best_model.fit(X_train, y_train)\n",
    "    # Save best model\n",
    "    joblib.dump(best_model, \"best_model.pkl\")\n",
    "    print(\"[INFO] Best model saved as best_model.pkl\")\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    if task == \"classification\":\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Test Accuracy: {acc:.4f}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(\"Confusion Matrix:\\n\", cm)\n",
    "        if acc < min_score:\n",
    "            print(f\"\\n[WARNING] Accuracy is less than {min_score*100:.1f}%!\")\n",
    "    else:\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(f\"Test R2: {r2:.4f}, MSE: {mse:.4f}\")\n",
    "        if r2 < min_score:\n",
    "            print(f\"\\n[WARNING] R2 is less than {min_score*100:.1f}%!\")\n",
    "\n",
    "    if hasattr(best_model, \"feature_importances_\"):\n",
    "        importances = best_model.feature_importances_\n",
    "        idx = numba_argsort(importances)[::-1][:10]\n",
    "        print_step(\"Top 10 Features\")\n",
    "        for i in idx:\n",
    "            print(f\"{feat_names[i]}: {importances[i]:.4f}\")\n",
    "\n",
    "    print(f\"\\n[INFO] Total pipeline time: {time.time()-t0:.1f} sec.\")\n",
    "\n",
    "    print_step(\"Summary Table (All Models)\")\n",
    "    df_results = pd.DataFrame(results_table)\n",
    "    df_results = df_results.sort_values(\"CV_Mean\", ascending=False)\n",
    "    display(df_results.reset_index(drop=True))\n",
    "\n",
    "\n",
    "auto_ml_ultra(df, target_col, time_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3612f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional models\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "    _LGBM_OK = True\n",
    "except: _LGBM_OK = False\n",
    "try:\n",
    "    from xgboost import XGBClassifier, XGBRegressor\n",
    "    _XGB_OK = True\n",
    "except: _XGB_OK = False\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "    _CAT_OK = True\n",
    "except: _CAT_OK = False\n",
    "\n",
    "from numba import njit\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, PowerTransformer, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, mutual_info_classif\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "@njit\n",
    "def numba_argsort(arr):\n",
    "    return np.argsort(arr)\n",
    "\n",
    "def print_step(msg):\n",
    "    print(f\"\\n{'='*10} {msg} {'='*10}\")\n",
    "\n",
    "def detect_problem_type(df, target_col, time_col=None):\n",
    "    if time_col and time_col in df.columns:\n",
    "        try:\n",
    "            pd.to_datetime(df[time_col])\n",
    "            return \"time_series\"\n",
    "        except: pass\n",
    "    y = df[target_col]\n",
    "    if pd.api.types.is_numeric_dtype(y):\n",
    "        if (y.dropna() % 1 == 0).all() and y.nunique(dropna=True) <= 20:\n",
    "            return \"classification\"\n",
    "        else:\n",
    "            return \"regression\"\n",
    "    return \"classification\"\n",
    "\n",
    "def enhanced_feature_engineering(X, y, task_type=\"classification\"):\n",
    "    \"\"\"Advanced feature engineering\"\"\"\n",
    "    print(\"[INFO] Applying advanced feature engineering...\")\n",
    "    \n",
    "    # 1. Feature Selection\n",
    "    if task_type == \"classification\":\n",
    "        # Select top features using mutual information\n",
    "        selector = SelectKBest(mutual_info_classif, k=min(50, X.shape[1]))\n",
    "        X_selected = selector.fit_transform(X, y)\n",
    "        print(f\"[INFO] Selected {X_selected.shape[1]} features using mutual information\")\n",
    "    else:\n",
    "        from sklearn.feature_selection import mutual_info_regression\n",
    "        selector = SelectKBest(mutual_info_regression, k=min(50, X.shape[1]))\n",
    "        X_selected = selector.fit_transform(X, y)\n",
    "        print(f\"[INFO] Selected {X_selected.shape[1]} features using mutual information\")\n",
    "    \n",
    "    # 2. Polynomial Features (for small datasets)\n",
    "    if X_selected.shape[1] <= 20 and X_selected.shape[0] < 5000:\n",
    "        print(\"[INFO] Adding polynomial features...\")\n",
    "        poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "        X_poly = poly.fit_transform(X_selected)\n",
    "        \n",
    "        # Select best polynomial features\n",
    "        if task_type == \"classification\":\n",
    "            poly_selector = SelectKBest(f_classif, k=min(100, X_poly.shape[1]))\n",
    "        else:\n",
    "            from sklearn.feature_selection import f_regression\n",
    "            poly_selector = SelectKBest(f_regression, k=min(100, X_poly.shape[1]))\n",
    "        \n",
    "        X_final = poly_selector.fit_transform(X_poly, y)\n",
    "        print(f\"[INFO] Final feature count after polynomial: {X_final.shape[1]}\")\n",
    "        return X_final, (selector, poly, poly_selector)\n",
    "    \n",
    "    return X_selected, (selector,)\n",
    "\n",
    "def preprocess_data(df, target_col, time_col=None):\n",
    "    df = df.copy()\n",
    "    if time_col and time_col in df.columns:\n",
    "        df = df.sort_values(time_col)\n",
    "    \n",
    "    cat_cols = df.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col in cat_cols: cat_cols.remove(target_col)\n",
    "    if target_col in num_cols: num_cols.remove(target_col)\n",
    "    \n",
    "    # Basic preprocessing without feature selection in pipeline\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"power\", PowerTransformer(method='yeo-johnson')),\n",
    "        (\"scaler\", RobustScaler())\n",
    "    ])\n",
    "    \n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False, drop='first'))\n",
    "    ]) if cat_cols else \"drop\"\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols)\n",
    "    ])\n",
    "    \n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Apply basic preprocessing\n",
    "    X_proc = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # Detect task type for advanced feature engineering\n",
    "    task_type = detect_problem_type(df, target_col)\n",
    "    \n",
    "    # Apply advanced feature engineering\n",
    "    X_final, feature_selectors = enhanced_feature_engineering(X_proc, y, task_type)\n",
    "    \n",
    "    # Generate feature names\n",
    "    feat_names = [f\"feature_{i}\" for i in range(X_final.shape[1])]\n",
    "    \n",
    "    return X_final, y, feat_names, preprocessor, feature_selectors\n",
    "\n",
    "def get_models(task, n_samples=None):\n",
    "    heavy_iter = 1000 if n_samples is not None and n_samples > 50000 else 5000\n",
    "    mlp_iter = 300 if n_samples is not None and n_samples > 50000 else 1000\n",
    "    lgbm_estimators = 200 if n_samples is not None and n_samples > 50000 else 500\n",
    "    xgb_estimators = 200 if n_samples is not None and n_samples > 50000 else 500\n",
    "    cat_estimators = 200 if n_samples is not None and n_samples > 50000 else 500\n",
    "\n",
    "    models = []\n",
    "    if task == \"classification\":\n",
    "        models += [\n",
    "            # Enhanced Tree Models\n",
    "            (\"RandomForest_Tuned\", RandomForestClassifier(\n",
    "                n_estimators=lgbm_estimators, max_depth=20, min_samples_split=3, \n",
    "                min_samples_leaf=1, max_features='sqrt', class_weight='balanced_subsample',\n",
    "                bootstrap=True, oob_score=True, n_jobs=-1, random_state=42)),\n",
    "            \n",
    "            (\"ExtraTrees_Tuned\", ExtraTreesClassifier(\n",
    "                n_estimators=lgbm_estimators, max_depth=25, min_samples_split=2,\n",
    "                min_samples_leaf=1, max_features='sqrt', class_weight='balanced_subsample',\n",
    "                bootstrap=True, oob_score=True, n_jobs=-1, random_state=42)),\n",
    "            \n",
    "            (\"GradientBoosting_Tuned\", GradientBoostingClassifier(\n",
    "                n_estimators=lgbm_estimators, learning_rate=0.05, max_depth=10, \n",
    "                subsample=0.9, max_features='sqrt', random_state=42)),\n",
    "            \n",
    "            (\"HistGB_Tuned\", HistGradientBoostingClassifier(\n",
    "                max_iter=lgbm_estimators, learning_rate=0.05, max_depth=12, \n",
    "                max_bins=255, class_weight='balanced', random_state=42)),\n",
    "            \n",
    "            # Enhanced Linear Models\n",
    "            (\"LogisticRegression_Tuned\", LogisticRegression(\n",
    "                max_iter=heavy_iter, C=0.01, class_weight='balanced', \n",
    "                penalty='elasticnet', l1_ratio=0.3, solver='saga', n_jobs=-1, random_state=42)),\n",
    "            \n",
    "            (\"SGD_Tuned\", SGDClassifier(\n",
    "                max_iter=heavy_iter, alpha=0.0001, class_weight='balanced', \n",
    "                loss='log_loss', penalty='elasticnet', l1_ratio=0.2, \n",
    "                learning_rate='adaptive', eta0=0.01, random_state=42)),\n",
    "            \n",
    "            (\"LinearSVC_Tuned\", LinearSVC(\n",
    "                max_iter=heavy_iter, C=0.01, class_weight='balanced', \n",
    "                penalty='l2', loss='squared_hinge', dual=False, random_state=42)),\n",
    "            \n",
    "            # Enhanced Ensemble Models\n",
    "            (\"AdaBoost_Tuned\", AdaBoostClassifier(\n",
    "                base_estimator=DecisionTreeClassifier(max_depth=3, class_weight='balanced'),\n",
    "                n_estimators=lgbm_estimators, learning_rate=0.5, random_state=42)),\n",
    "            \n",
    "            (\"Bagging_Tuned\", BaggingClassifier(\n",
    "                base_estimator=DecisionTreeClassifier(max_depth=15, class_weight='balanced'),\n",
    "                n_estimators=200, max_samples=0.7, max_features=0.8, \n",
    "                bootstrap=True, bootstrap_features=True, n_jobs=-1, random_state=42)),\n",
    "            \n",
    "            # Enhanced Neural Network\n",
    "            (\"MLP_Tuned\", MLPClassifier(\n",
    "                hidden_layer_sizes=(256,128,64,32), max_iter=mlp_iter, alpha=0.0001, \n",
    "                learning_rate='adaptive', early_stopping=True, validation_fraction=0.1,\n",
    "                beta_1=0.9, beta_2=0.999, epsilon=1e-8, random_state=42)),\n",
    "            \n",
    "            # Enhanced KNN\n",
    "            (\"KNN_Tuned\", KNeighborsClassifier(\n",
    "                n_neighbors=11, weights='distance', metric='minkowski', \n",
    "                p=2, algorithm='ball_tree', n_jobs=-1)),\n",
    "            \n",
    "            # Other models\n",
    "            (\"GaussianNB\", GaussianNB()),\n",
    "            (\"LDA_Tuned\", LinearDiscriminantAnalysis(solver='svd', shrinkage=None)),\n",
    "            (\"QDA\", QuadraticDiscriminantAnalysis()),\n",
    "        ]\n",
    "        \n",
    "        # Enhanced Gradient Boosting Models\n",
    "        if _LGBM_OK: \n",
    "            models.append((\"LightGBM_Tuned\", LGBMClassifier(\n",
    "                n_estimators=lgbm_estimators, learning_rate=0.05, max_depth=15, \n",
    "                num_leaves=100, subsample=0.8, colsample_bytree=0.8, \n",
    "                reg_alpha=0.1, reg_lambda=0.1, class_weight='balanced', \n",
    "                n_jobs=-1, random_state=42, verbosity=-1)))\n",
    "        \n",
    "        if _XGB_OK:  \n",
    "            models.append((\"XGBoost_Tuned\", XGBClassifier(\n",
    "                n_estimators=xgb_estimators, learning_rate=0.05, max_depth=12, \n",
    "                subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=0.1,\n",
    "                scale_pos_weight=1, n_jobs=-1, random_state=42, \n",
    "                verbosity=0, use_label_encoder=False, eval_metric='mlogloss')))\n",
    "        \n",
    "        if _CAT_OK:  \n",
    "            models.append((\"CatBoost_Tuned\", CatBoostClassifier(\n",
    "                iterations=cat_estimators, learning_rate=0.05, depth=10, \n",
    "                l2_leaf_reg=3, class_weights='Balanced', \n",
    "                verbose=0, random_state=42)))\n",
    "    \n",
    "    # Similar enhancements for regression...\n",
    "    else:\n",
    "        models += [\n",
    "            (\"RandomForest_Tuned\", RandomForestRegressor(\n",
    "                n_estimators=lgbm_estimators, max_depth=20, min_samples_split=3, \n",
    "                min_samples_leaf=1, max_features='sqrt', n_jobs=-1, random_state=42)),\n",
    "            \n",
    "            (\"ExtraTrees_Tuned\", ExtraTreesRegressor(\n",
    "                n_estimators=lgbm_estimators, max_depth=25, min_samples_split=2,\n",
    "                min_samples_leaf=1, max_features='sqrt', n_jobs=-1, random_state=42)),\n",
    "            \n",
    "            (\"GradientBoosting_Tuned\", GradientBoostingRegressor(\n",
    "                n_estimators=lgbm_estimators, learning_rate=0.05, max_depth=10, \n",
    "                subsample=0.9, max_features='sqrt', random_state=42)),\n",
    "            \n",
    "            (\"LinearRegression\", LinearRegression()),\n",
    "            (\"Ridge_Tuned\", Ridge(alpha=10.0)),\n",
    "            (\"Lasso_Tuned\", Lasso(alpha=0.01)),\n",
    "            (\"ElasticNet_Tuned\", ElasticNet(alpha=0.01, l1_ratio=0.3)),\n",
    "        ]\n",
    "        \n",
    "        if _LGBM_OK: \n",
    "            models.append((\"LightGBM_Tuned\", LGBMRegressor(\n",
    "                n_estimators=lgbm_estimators, learning_rate=0.05, max_depth=15, \n",
    "                num_leaves=100, subsample=0.8, colsample_bytree=0.8,\n",
    "                n_jobs=-1, random_state=42, verbosity=-1)))\n",
    "    \n",
    "    return models\n",
    "\n",
    "def auto_ml_ultra(df, target_col, time_col=None, min_score=0.88):\n",
    "    import joblib\n",
    "    t0 = time.time()\n",
    "    print_step(\"Detecting problem type\")\n",
    "    task = detect_problem_type(df, target_col, time_col)\n",
    "    print(f\"[INFO] Task: {task}\")\n",
    "\n",
    "    print_step(\"Advanced Preprocessing & Feature Engineering\")\n",
    "    X_proc, y, feat_names, preprocessor, feature_selectors = preprocess_data(df, target_col, time_col)\n",
    "    print(f\"[INFO] Features: {X_proc.shape[1]} | Samples: {X_proc.shape[0]}\")\n",
    "\n",
    "    print_step(\"Data Balancing\")\n",
    "    # Apply SMOTE for better class balance\n",
    "    if task == \"classification\":\n",
    "        class_counts = pd.Series(y).value_counts()\n",
    "        print(f\"[INFO] Original class distribution:\\n{class_counts}\")\n",
    "        \n",
    "        if len(class_counts) > 2:  # Multi-class\n",
    "            smote = SMOTE(random_state=42, k_neighbors=min(5, min(class_counts)-1))\n",
    "        else:  # Binary\n",
    "            smote = SMOTE(random_state=42)\n",
    "        \n",
    "        X_proc, y = smote.fit_resample(X_proc, y)\n",
    "        print(f\"[INFO] After SMOTE: {X_proc.shape[0]} samples\")\n",
    "        print(f\"[INFO] New class distribution:\\n{pd.Series(y).value_counts()}\")\n",
    "\n",
    "    print_step(\"Splitting data\")\n",
    "    if task == \"classification\":\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_proc, y, test_size=0.15, random_state=42, stratify=y)\n",
    "        cv = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
    "        score_metric = \"f1_macro\"\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_proc, y, test_size=0.15, random_state=42)\n",
    "        cv = KFold(n_splits=7, shuffle=True, random_state=42)\n",
    "        score_metric = \"r2\"\n",
    "    print(f\"[INFO] Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "    print_step(\"Model selection & training\")\n",
    "    models = get_models(task, n_samples=X_train.shape[0])\n",
    "    results = []\n",
    "    results_table = []\n",
    "    \n",
    "    for idx, (name, model) in enumerate(models):\n",
    "        t1 = time.time()\n",
    "        try:\n",
    "            scores = cross_val_score(model, X_train, y_train, scoring=score_metric, cv=cv, n_jobs=-1)\n",
    "            score = scores.mean()\n",
    "            std = scores.std()\n",
    "            print(f\"[{idx+1:02d}/{len(models)}] {name}: CV {score_metric} = {score:.4f} ± {std:.4f} | Time: {time.time()-t1:.1f}s\")\n",
    "            results.append((score, name, model))\n",
    "            results_table.append({\n",
    "                \"Model\": name,\n",
    "                \"CV_Mean\": score,\n",
    "                \"CV_Std\": std,\n",
    "                \"Time_sec\": time.time()-t1\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx+1:02d}/{len(models)}] {name}: ERROR {e}\")\n",
    "            results_table.append({\n",
    "                \"Model\": name,\n",
    "                \"CV_Mean\": np.nan,\n",
    "                \"CV_Std\": np.nan,\n",
    "                \"Time_sec\": time.time()-t1,\n",
    "                \"Error\": str(e)\n",
    "            })\n",
    "    \n",
    "    results.sort(reverse=True)\n",
    "    top_models = [r for r in results if not np.isnan(r[0])][:10]\n",
    "    best_score, best_name, best_model = top_models[0]\n",
    "\n",
    "    print_step(\"Advanced Ensemble (Voting/Stacking)\")\n",
    "    try:\n",
    "        # Select diverse models for ensemble\n",
    "        diverse_models = []\n",
    "        model_types = set()\n",
    "        for score, name, model in top_models:\n",
    "            model_type = name.split('_')[0]\n",
    "            if model_type not in model_types or len(diverse_models) < 5:\n",
    "                diverse_models.append((score, name, model))\n",
    "                model_types.add(model_type)\n",
    "            if len(diverse_models) >= 8:\n",
    "                break\n",
    "\n",
    "        if task == \"classification\":\n",
    "            voting = VotingClassifier(\n",
    "                estimators=[(n, m) for _, n, m in diverse_models],\n",
    "                voting='soft',\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            stacking = StackingClassifier(\n",
    "                estimators=[(n, m) for _, n, m in diverse_models],\n",
    "                final_estimator=LogisticRegression(\n",
    "                    max_iter=5000, C=0.1, class_weight='balanced', random_state=42),\n",
    "                cv=5,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        else:\n",
    "            voting = VotingRegressor(\n",
    "                estimators=[(n, m) for _, n, m in diverse_models],\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            stacking = StackingRegressor(\n",
    "                estimators=[(n, m) for _, n, m in diverse_models],\n",
    "                final_estimator=Ridge(alpha=1.0),\n",
    "                cv=5,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "        for ens_name, ens_model in [(\"Voting_Advanced\", voting), (\"Stacking_Advanced\", stacking)]:\n",
    "            t1 = time.time()\n",
    "            scores = cross_val_score(\n",
    "                ens_model, X_train, y_train,\n",
    "                scoring=score_metric, cv=cv, n_jobs=-1\n",
    "            )\n",
    "            score = scores.mean()\n",
    "            std = scores.std()\n",
    "            elapsed = time.time() - t1\n",
    "\n",
    "            print(f\"[Ensemble] {ens_name}: CV {score_metric} = {score:.4f} ± {std:.4f} | Time: {elapsed:.1f}s\")\n",
    "\n",
    "            results.append((score, ens_name, ens_model))\n",
    "            results_table.append({\n",
    "                \"Model\": ens_name,\n",
    "                \"CV_Mean\": score,\n",
    "                \"CV_Std\": std,\n",
    "                \"Time_sec\": elapsed\n",
    "            })\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score, best_name, best_model = score, ens_name, ens_model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ensemble Error: {e}\")\n",
    "\n",
    "    print_step(f\"Best Model: {best_name} (CV {score_metric}: {best_score:.4f})\")\n",
    "    best_model.fit(X_train, y_train)\n",
    "    joblib.dump(best_model, \"best_model_enhanced.pkl\")\n",
    "    print(\"[INFO] Best model saved as best_model_enhanced.pkl\")\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    if task == \"classification\":\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Test Accuracy: {acc:.4f}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(\"Confusion Matrix:\\n\", cm)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "        \n",
    "        if acc < min_score:\n",
    "            print(f\"\\n[WARNING] Accuracy is less than {min_score*100:.1f}%!\")\n",
    "        else:\n",
    "            print(f\"\\n SUCCESS! Accuracy {acc:.4f} is above {min_score*100:.1f}%!\")\n",
    "    else:\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(f\"Test R2: {r2:.4f}, MSE: {mse:.4f}\")\n",
    "        if r2 < min_score:\n",
    "            print(f\"\\n[WARNING] R2 is less than {min_score*100:.1f}%!\")\n",
    "\n",
    "    print(f\"\\n[INFO] Total pipeline time: {time.time()-t0:.1f} sec.\")\n",
    "\n",
    "    print_step(\"Summary Table (All Models)\")\n",
    "    df_results = pd.DataFrame(results_table)\n",
    "    df_results = df_results.sort_values(\"CV_Mean\", ascending=False)\n",
    "    display(df_results.reset_index(drop=True))\n",
    "\n",
    "    return best_model, preprocessor, feature_selectors\n",
    "\n",
    "# Run the enhanced AutoML\n",
    "best_model, preprocessor, feature_selectors = auto_ml_ultra(df, target_col, time_col=None, min_score=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "    _LGBM_OK = True\n",
    "except: _LGBM_OK = False\n",
    "try:\n",
    "    from xgboost import XGBClassifier, XGBRegressor\n",
    "    _XGB_OK = True\n",
    "except: _XGB_OK = False\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "    _CAT_OK = True\n",
    "except: _CAT_OK = False\n",
    "\n",
    "from numba import njit\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, VarianceThreshold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "@njit\n",
    "def numba_argsort(arr):\n",
    "    return np.argsort(arr)\n",
    "\n",
    "def print_step(msg):\n",
    "    print(f\"\\n{'='*10} {msg} {'='*10}\")\n",
    "\n",
    "def detect_problem_type(df, target_col, time_col=None):\n",
    "    if time_col and time_col in df.columns:\n",
    "        try:\n",
    "            pd.to_datetime(df[time_col])\n",
    "            return \"time_series\"\n",
    "        except: pass\n",
    "    y = df[target_col]\n",
    "    if pd.api.types.is_numeric_dtype(y):\n",
    "        if (y.dropna() % 1 == 0).all() and y.nunique(dropna=True) <= 20:\n",
    "            return \"classification\"\n",
    "        else:\n",
    "            return \"regression\"\n",
    "    return \"classification\"\n",
    "\n",
    "def advanced_feature_engineering(X, y, task_type=\"classification\"):\n",
    "    \"\"\"Ultra advanced feature engineering for maximum performance\"\"\"\n",
    "    print(\"[INFO] Applying ultra-advanced feature engineering...\")\n",
    "    \n",
    "    original_features = X.shape[1]\n",
    "    \n",
    "    # 1. Remove low variance features\n",
    "    variance_selector = VarianceThreshold(threshold=0.01)\n",
    "    X_var = variance_selector.fit_transform(X)\n",
    "    print(f\"[INFO] Removed {original_features - X_var.shape[1]} low-variance features\")\n",
    "    \n",
    "    # 2. Feature scaling with multiple methods and selection\n",
    "    scalers = [\n",
    "        ('robust', RobustScaler()),\n",
    "        ('quantile', QuantileTransformer(output_distribution='normal')),\n",
    "        ('power', PowerTransformer(method='yeo-johnson'))\n",
    "    ]\n",
    "    \n",
    "    best_score = 0\n",
    "    best_X_scaled = X_var\n",
    "    best_scaler_name = 'none'\n",
    "    \n",
    "    for scaler_name, scaler in scalers:\n",
    "        try:\n",
    "            X_scaled = scaler.fit_transform(X_var)\n",
    "            \n",
    "            # Quick test with a simple model\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            from sklearn.model_selection import cross_val_score\n",
    "            \n",
    "            if task_type == \"classification\":\n",
    "                quick_model = LogisticRegression(max_iter=100, random_state=42)\n",
    "                scores = cross_val_score(quick_model, X_scaled, y, cv=3, scoring='f1_macro')\n",
    "            else:\n",
    "                from sklearn.linear_model import Ridge\n",
    "                quick_model = Ridge(random_state=42)\n",
    "                scores = cross_val_score(quick_model, X_scaled, y, cv=3, scoring='r2')\n",
    "            \n",
    "            score = scores.mean()\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_X_scaled = X_scaled\n",
    "                best_scaler_name = scaler_name\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(f\"[INFO] Best scaler: {best_scaler_name} with score: {best_score:.4f}\")\n",
    "    \n",
    "    # 3. Feature selection with multiple methods\n",
    "    if task_type == \"classification\":\n",
    "        # Mutual information\n",
    "        mi_selector = SelectKBest(mutual_info_classif, k=min(100, best_X_scaled.shape[1]))\n",
    "        X_mi = mi_selector.fit_transform(best_X_scaled, y)\n",
    "        \n",
    "        # F-score\n",
    "        f_selector = SelectKBest(f_classif, k=min(80, best_X_scaled.shape[1]))\n",
    "        X_f = f_selector.fit_transform(best_X_scaled, y)\n",
    "        \n",
    "        # Combine both selections\n",
    "        mi_mask = mi_selector.get_support()\n",
    "        f_mask = f_selector.get_support()\n",
    "        combined_mask = mi_mask | f_mask  # Union of both selections\n",
    "        \n",
    "        X_selected = best_X_scaled[:, combined_mask]\n",
    "        print(f\"[INFO] Selected {X_selected.shape[1]} features using combined MI+F-score\")\n",
    "    else:\n",
    "        from sklearn.feature_selection import mutual_info_regression, f_regression\n",
    "        mi_selector = SelectKBest(mutual_info_regression, k=min(100, best_X_scaled.shape[1]))\n",
    "        X_selected = mi_selector.fit_transform(best_X_scaled, y)\n",
    "        print(f\"[INFO] Selected {X_selected.shape[1]} features using mutual information\")\n",
    "    \n",
    "    # 4. Dimensionality reduction if too many features\n",
    "    if X_selected.shape[1] > 200:\n",
    "        print(\"[INFO] Applying PCA for dimensionality reduction...\")\n",
    "        pca = PCA(n_components=min(200, X_selected.shape[0]//2), random_state=42)\n",
    "        X_final = pca.fit_transform(X_selected)\n",
    "        print(f\"[INFO] PCA reduced features to {X_final.shape[1]} (explained variance: {pca.explained_variance_ratio_.sum():.3f})\")\n",
    "        return X_final, (variance_selector, best_scaler_name, mi_selector, pca)\n",
    "    \n",
    "    return X_selected, (variance_selector, best_scaler_name, mi_selector)\n",
    "\n",
    "def preprocess_data(df, target_col, time_col=None):\n",
    "    df = df.copy()\n",
    "    if time_col and time_col in df.columns:\n",
    "        df = df.sort_values(time_col)\n",
    "    \n",
    "    cat_cols = df.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col in cat_cols: cat_cols.remove(target_col)\n",
    "    if target_col in num_cols: num_cols.remove(target_col)\n",
    "    \n",
    "    # Advanced preprocessing\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ])\n",
    "    \n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False, drop='first'))\n",
    "    ]) if cat_cols else \"drop\"\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols)\n",
    "    ])\n",
    "    \n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Apply basic preprocessing\n",
    "    X_proc = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # Detect task type for advanced feature engineering\n",
    "    task_type = detect_problem_type(df, target_col)\n",
    "    \n",
    "    # Apply ultra-advanced feature engineering\n",
    "    X_final, feature_selectors = advanced_feature_engineering(X_proc, y, task_type)\n",
    "    \n",
    "    # Generate feature names\n",
    "    feat_names = [f\"engineered_feature_{i}\" for i in range(X_final.shape[1])]\n",
    "    \n",
    "    return X_final, y, feat_names, preprocessor, feature_selectors\n",
    "\n",
    "def get_ultra_models(task, n_samples=None):\n",
    "    \"\"\"Get ultra-tuned models for maximum performance\"\"\"\n",
    "    models = []\n",
    "    \n",
    "    if task == \"classification\":\n",
    "        models += [\n",
    "            # Ultra-tuned Random Forest\n",
    "            (\"UltraRandomForest\", RandomForestClassifier(\n",
    "                n_estimators=800, max_depth=None, min_samples_split=2, \n",
    "                min_samples_leaf=1, max_features='log2', class_weight='balanced',\n",
    "                bootstrap=True, oob_score=True, criterion='gini',\n",
    "                max_samples=0.8, n_jobs=-1, random_state=42)),\n",
    "            \n",
    "            # Ultra-tuned Extra Trees\n",
    "            (\"UltraExtraTrees\", ExtraTreesClassifier(\n",
    "                n_estimators=800, max_depth=None, min_samples_split=2,\n",
    "                min_samples_leaf=1, max_features='sqrt', class_weight='balanced',\n",
    "                bootstrap=True, oob_score=True, criterion='entropy',\n",
    "                max_samples=0.9, n_jobs=-1, random_state=42)),\n",
    "            \n",
    "            # Ultra-tuned Gradient Boosting\n",
    "            (\"UltraGradBoost\", GradientBoostingClassifier(\n",
    "                n_estimators=600, learning_rate=0.02, max_depth=12, \n",
    "                subsample=0.85, max_features='sqrt', min_samples_split=3,\n",
    "                min_samples_leaf=2, validation_fraction=0.1, \n",
    "                n_iter_no_change=50, random_state=42)),\n",
    "            \n",
    "            # Ultra-tuned HistGradient\n",
    "            (\"UltraHistGB\", HistGradientBoostingClassifier(\n",
    "                max_iter=600, learning_rate=0.02, max_depth=15, \n",
    "                max_bins=255, class_weight='balanced', \n",
    "                validation_fraction=0.1, n_iter_no_change=50,\n",
    "                random_state=42)),\n",
    "            \n",
    "            # Multiple Logistic Regression variants\n",
    "            (\"LogisticL1\", LogisticRegression(\n",
    "                max_iter=10000, C=0.1, class_weight='balanced', \n",
    "                penalty='l1', solver='liblinear', random_state=42)),\n",
    "            \n",
    "            (\"LogisticL2\", LogisticRegression(\n",
    "                max_iter=10000, C=1.0, class_weight='balanced', \n",
    "                penalty='l2', solver='lbfgs', random_state=42)),\n",
    "            \n",
    "            (\"LogisticElastic\", LogisticRegression(\n",
    "                max_iter=10000, C=0.5, class_weight='balanced', \n",
    "                penalty='elasticnet', l1_ratio=0.5, solver='saga', random_state=42)),\n",
    "            \n",
    "            # Enhanced SVM\n",
    "            (\"LinearSVM\", LinearSVC(\n",
    "                C=0.1, class_weight='balanced', max_iter=10000,\n",
    "                penalty='l2', loss='squared_hinge', dual=False, random_state=42)),\n",
    "            \n",
    "            # Multiple KNN variants\n",
    "            (\"KNN_Uniform\", KNeighborsClassifier(\n",
    "                n_neighbors=15, weights='uniform', metric='euclidean', n_jobs=-1)),\n",
    "            \n",
    "            (\"KNN_Distance\", KNeighborsClassifier(\n",
    "                n_neighbors=21, weights='distance', metric='manhattan', n_jobs=-1)),\n",
    "            \n",
    "            # Enhanced Neural Networks\n",
    "            (\"MLP_Large\", MLPClassifier(\n",
    "                hidden_layer_sizes=(512, 256, 128, 64), max_iter=2000, alpha=0.001, \n",
    "                learning_rate='adaptive', early_stopping=True, validation_fraction=0.15,\n",
    "                beta_1=0.9, beta_2=0.999, epsilon=1e-8, random_state=42)),\n",
    "            \n",
    "            (\"MLP_Deep\", MLPClassifier(\n",
    "                hidden_layer_sizes=(256, 128, 64, 32, 16), max_iter=2000, alpha=0.01, \n",
    "                learning_rate='adaptive', early_stopping=True, validation_fraction=0.15,\n",
    "                beta_1=0.9, beta_2=0.999, random_state=42)),\n",
    "            \n",
    "            # Naive Bayes variants\n",
    "            (\"GaussianNB\", GaussianNB()),\n",
    "            \n",
    "            # Discriminant Analysis\n",
    "            (\"LDA\", LinearDiscriminantAnalysis(solver='svd')),\n",
    "            (\"QDA\", QuadraticDiscriminantAnalysis()),\n",
    "        ]\n",
    "        \n",
    "        # Ultra Gradient Boosting Models\n",
    "        if _LGBM_OK: \n",
    "            models.extend([\n",
    "                (\"UltraLightGBM_1\", LGBMClassifier(\n",
    "                    n_estimators=800, learning_rate=0.02, max_depth=20, \n",
    "                    num_leaves=150, subsample=0.8, colsample_bytree=0.8, \n",
    "                    reg_alpha=0.1, reg_lambda=0.1, class_weight='balanced', \n",
    "                    boosting_type='gbdt', objective='multiclass',\n",
    "                    n_jobs=-1, random_state=42, verbosity=-1)),\n",
    "                \n",
    "                (\"UltraLightGBM_2\", LGBMClassifier(\n",
    "                    n_estimators=1000, learning_rate=0.015, max_depth=25, \n",
    "                    num_leaves=200, subsample=0.85, colsample_bytree=0.85, \n",
    "                    reg_alpha=0.05, reg_lambda=0.05, class_weight='balanced',\n",
    "                    boosting_type='dart', objective='multiclass',\n",
    "                    n_jobs=-1, random_state=42, verbosity=-1))\n",
    "            ])\n",
    "        \n",
    "        if _XGB_OK:  \n",
    "            models.extend([\n",
    "                (\"UltraXGBoost_1\", XGBClassifier(\n",
    "                    n_estimators=800, learning_rate=0.02, max_depth=15, \n",
    "                    subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=0.1,\n",
    "                    objective='multi:softprob', eval_metric='mlogloss',\n",
    "                    n_jobs=-1, random_state=42, verbosity=0, use_label_encoder=False)),\n",
    "                \n",
    "                (\"UltraXGBoost_2\", XGBClassifier(\n",
    "                    n_estimators=1000, learning_rate=0.015, max_depth=18, \n",
    "                    subsample=0.85, colsample_bytree=0.85, reg_alpha=0.05, reg_lambda=0.05,\n",
    "                    objective='multi:softprob', eval_metric='mlogloss',\n",
    "                    n_jobs=-1, random_state=42, verbosity=0, use_label_encoder=False))\n",
    "            ])\n",
    "    \n",
    "    return models\n",
    "\n",
    "def ultra_ensemble(top_models, task=\"classification\"):\n",
    "    \"\"\"Create multiple ensemble variants\"\"\"\n",
    "    ensembles = []\n",
    "    \n",
    "    if task == \"classification\":\n",
    "        # Diverse model selection\n",
    "        diverse_models = []\n",
    "        used_types = set()\n",
    "        \n",
    "        for score, name, model in top_models:\n",
    "            model_type = name.split('_')[0] if '_' in name else name\n",
    "            if model_type not in used_types or len(diverse_models) < 6:\n",
    "                diverse_models.append((score, name, model))\n",
    "                used_types.add(model_type)\n",
    "            if len(diverse_models) >= 12:\n",
    "                break\n",
    "        \n",
    "        # Multiple voting classifiers\n",
    "        ensembles.extend([\n",
    "            (\"VotingClassifier_Soft\", VotingClassifier(\n",
    "                estimators=[(n, m) for _, n, m in diverse_models[:8]],\n",
    "                voting='soft', n_jobs=-1)),\n",
    "            \n",
    "            (\"VotingClassifier_Hard\", VotingClassifier(\n",
    "                estimators=[(n, m) for _, n, m in diverse_models[:8]],\n",
    "                voting='hard', n_jobs=-1)),\n",
    "        ])\n",
    "        \n",
    "        # Multiple stacking classifiers\n",
    "        ensembles.extend([\n",
    "            (\"StackingClassifier_LR\", StackingClassifier(\n",
    "                estimators=[(n, m) for _, n, m in diverse_models[:10]],\n",
    "                final_estimator=LogisticRegression(max_iter=10000, C=0.1, class_weight='balanced'),\n",
    "                cv=5, n_jobs=-1)),\n",
    "            \n",
    "            (\"StackingClassifier_RF\", StackingClassifier(\n",
    "                estimators=[(n, m) for _, n, m in diverse_models[:10]],\n",
    "                final_estimator=RandomForestClassifier(n_estimators=200, class_weight='balanced'),\n",
    "                cv=5, n_jobs=-1)),\n",
    "            \n",
    "            (\"StackingClassifier_MLP\", StackingClassifier(\n",
    "                estimators=[(n, m) for _, n, m in diverse_models[:8]],\n",
    "                final_estimator=MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=2000),\n",
    "                cv=5, n_jobs=-1)),\n",
    "        ])\n",
    "    \n",
    "    return ensembles\n",
    "\n",
    "def auto_ml_ultra_v2(df, target_col, time_col=None, min_score=0.90):\n",
    "    import joblib\n",
    "    t0 = time.time()\n",
    "    print_step(\"Ultra ML Pipeline Starting\")\n",
    "    \n",
    "    task = detect_problem_type(df, target_col, time_col)\n",
    "    print(f\"[INFO] Task: {task}\")\n",
    "\n",
    "    print_step(\"Ultra-Advanced Preprocessing & Feature Engineering\")\n",
    "    X_proc, y, feat_names, preprocessor, feature_selectors = preprocess_data(df, target_col, time_col)\n",
    "    print(f\"[INFO] Features: {X_proc.shape[1]} | Samples: {X_proc.shape[0]}\")\n",
    "\n",
    "    print_step(\"Advanced Data Balancing\")\n",
    "    if task == \"classification\":\n",
    "        class_counts = pd.Series(y).value_counts()\n",
    "        print(f\"[INFO] Original class distribution:\\n{class_counts}\")\n",
    "        \n",
    "        # Multiple balancing strategies\n",
    "        balancing_methods = [\n",
    "            (\"SMOTE\", SMOTE(random_state=42, k_neighbors=min(5, min(class_counts)-1))),\n",
    "            (\"ADASYN\", ADASYN(random_state=42, n_neighbors=min(5, min(class_counts)-1))),\n",
    "            (\"BorderlineSMOTE\", BorderlineSMOTE(random_state=42, k_neighbors=min(5, min(class_counts)-1))),\n",
    "            (\"SMOTEENN\", SMOTEENN(random_state=42))\n",
    "        ]\n",
    "        \n",
    "        best_balance_score = 0\n",
    "        best_X_balanced = X_proc\n",
    "        best_y_balanced = y\n",
    "        best_method_name = \"Original\"\n",
    "        \n",
    "        for method_name, method in balancing_methods:\n",
    "            try:\n",
    "                X_bal, y_bal = method.fit_resample(X_proc, y)\n",
    "                \n",
    "                # Quick evaluation\n",
    "                from sklearn.model_selection import cross_val_score\n",
    "                from sklearn.ensemble import RandomForestClassifier\n",
    "                \n",
    "                quick_rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "                scores = cross_val_score(quick_rf, X_bal, y_bal, cv=3, scoring='f1_macro')\n",
    "                score = scores.mean()\n",
    "                \n",
    "                print(f\"[INFO] {method_name}: F1-macro = {score:.4f}, samples = {len(y_bal)}\")\n",
    "                \n",
    "                if score > best_balance_score:\n",
    "                    best_balance_score = score\n",
    "                    best_X_balanced = X_bal\n",
    "                    best_y_balanced = y_bal\n",
    "                    best_method_name = method_name\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"[INFO] {method_name} failed: {e}\")\n",
    "        \n",
    "        print(f\"[INFO] Best balancing method: {best_method_name}\")\n",
    "        X_proc, y = best_X_balanced, best_y_balanced\n",
    "\n",
    "    print_step(\"Splitting data\")\n",
    "    if task == \"classification\":\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_proc, y, test_size=0.1, random_state=42, stratify=y)  # Smaller test set\n",
    "        cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)  # More folds\n",
    "        score_metric = \"f1_macro\"\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_proc, y, test_size=0.1, random_state=42)\n",
    "        cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        score_metric = \"r2\"\n",
    "    print(f\"[INFO] Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "    print_step(\"Ultra Model Training\")\n",
    "    models = get_ultra_models(task, n_samples=X_train.shape[0])\n",
    "    results = []\n",
    "    results_table = []\n",
    "    \n",
    "    for idx, (name, model) in enumerate(models):\n",
    "        t1 = time.time()\n",
    "        try:\n",
    "            scores = cross_val_score(model, X_train, y_train, scoring=score_metric, cv=cv, n_jobs=-1)\n",
    "            score = scores.mean()\n",
    "            std = scores.std()\n",
    "            print(f\"[{idx+1:02d}/{len(models)}] {name}: CV {score_metric} = {score:.4f} ± {std:.4f} | Time: {time.time()-t1:.1f}s\")\n",
    "            results.append((score, name, model))\n",
    "            results_table.append({\n",
    "                \"Model\": name,\n",
    "                \"CV_Mean\": score,\n",
    "                \"CV_Std\": std,\n",
    "                \"Time_sec\": time.time()-t1\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx+1:02d}/{len(models)}] {name}: ERROR {e}\")\n",
    "            results_table.append({\n",
    "                \"Model\": name,\n",
    "                \"CV_Mean\": np.nan,\n",
    "                \"CV_Std\": np.nan,\n",
    "                \"Time_sec\": time.time()-t1,\n",
    "                \"Error\": str(e)\n",
    "            })\n",
    "    \n",
    "    results.sort(reverse=True)\n",
    "    top_models = [r for r in results if not np.isnan(r[0])][:15]  # More models for ensemble\n",
    "    best_score, best_name, best_model = top_models[0]\n",
    "\n",
    "    print_step(\"Ultra Ensemble Creation\")\n",
    "    try:\n",
    "        ensembles = ultra_ensemble(top_models, task)\n",
    "        \n",
    "        for ens_name, ens_model in ensembles:\n",
    "            t1 = time.time()\n",
    "            try:\n",
    "                scores = cross_val_score(\n",
    "                    ens_model, X_train, y_train,\n",
    "                    scoring=score_metric, cv=cv, n_jobs=-1\n",
    "                )\n",
    "                score = scores.mean()\n",
    "                std = scores.std()\n",
    "                elapsed = time.time() - t1\n",
    "\n",
    "                print(f\"[Ensemble] {ens_name}: CV {score_metric} = {score:.4f} ± {std:.4f} | Time: {elapsed:.1f}s\")\n",
    "\n",
    "                results.append((score, ens_name, ens_model))\n",
    "                results_table.append({\n",
    "                    \"Model\": ens_name,\n",
    "                    \"CV_Mean\": score,\n",
    "                    \"CV_Std\": std,\n",
    "                    \"Time_sec\": elapsed\n",
    "                })\n",
    "\n",
    "                if score > best_score:\n",
    "                    best_score, best_name, best_model = score, ens_name, ens_model\n",
    "            except Exception as e:\n",
    "                print(f\"[Ensemble] {ens_name}: ERROR {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ensemble Error: {e}\")\n",
    "\n",
    "    print_step(f\" FINAL BEST MODEL: {best_name} (CV {score_metric}: {best_score:.4f})\")\n",
    "    best_model.fit(X_train, y_train)\n",
    "    joblib.dump(best_model, \"ultra_best_model.pkl\")\n",
    "    print(\"[INFO] Ultra best model saved as ultra_best_model.pkl\")\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    if task == \"classification\":\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\" FINAL TEST ACCURACY: {acc:.4f}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(\"Confusion Matrix:\\n\", cm)\n",
    "        \n",
    "        # Enhanced confusion matrix plot\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "        plt.title(f'Confusion Matrix - {best_name}\\nAccuracy: {acc:.4f}', fontsize=16)\n",
    "        plt.ylabel('True Label', fontsize=14)\n",
    "        plt.xlabel('Predicted Label', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        if acc >= min_score:\n",
    "            print(f\"\\n MISSION ACCOMPLISHED! \")\n",
    "            print(f\" Accuracy {acc:.4f} is ABOVE target {min_score*100:.1f}%!\")\n",
    "            print(f\" Best model: {best_name}\")\n",
    "        else:\n",
    "            print(f\"\\n Close but not quite there...\")\n",
    "            print(f\" Accuracy {acc:.4f} is below target {min_score*100:.1f}%\")\n",
    "            print(f\" But we improved significantly with {best_name}!\")\n",
    "\n",
    "    print(f\"\\n Total ultra pipeline time: {time.time()-t0:.1f} seconds\")\n",
    "\n",
    "    print_step(\" FINAL RESULTS TABLE\")\n",
    "    df_results = pd.DataFrame(results_table)\n",
    "    df_results = df_results.sort_values(\"CV_Mean\", ascending=False)\n",
    "    display(df_results.reset_index(drop=True))\n",
    "\n",
    "    return best_model, preprocessor, feature_selectors\n",
    "\n",
    "\n",
    "print(\" Starting ULTRA AutoML Pipeline for Maximum Performance! \")\n",
    "best_model, preprocessor, feature_selectors = auto_ml_ultra_v2(df, target_col, time_col=None, min_score=0.90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2b5984",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "## 6. Save Project Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511e6196",
   "metadata": {},
   "source": [
    "### Generate requirements.txt File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e6887e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "output_file = \"requirements.txt\"\n",
    "packages = [\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"numba\",\n",
    "    \"scikit-learn\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\"\n",
    "]\n",
    "with open(output_file, \"w\") as f:\n",
    "    for package in packages:\n",
    "        try:\n",
    "            version = pkg_resources.get_distribution(package).version\n",
    "            f.write(f\"{package}=={version}\\n\")\n",
    "        except pkg_resources.DistributionNotFound:\n",
    "            f.write(f\"{package}\\n\")  \n",
    "print(output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
